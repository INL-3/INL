{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34859d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip -q install -U transformers datasets accelerate peft pandas sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5cf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip -q install -U tiktoken protobuf sentencepiece\n",
    "!pip -q install -U transformers datasets accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfdda254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aleksander\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "BASE_MODEL = \"allegro/plt5-base\"  \n",
    "\n",
    "OUT_DIR = \"plt5-lora-qa\"\n",
    "MAX_SOURCE_LEN = 384\n",
    "MAX_TARGET_LEN = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d43ffff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2554, 284)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_answer(a: str) -> str:\n",
    "    return str(a).strip().strip('\"').strip(\"„”\").strip().rstrip(\" .,:;\")\n",
    "\n",
    "def load_contexts_csv(path) -> Dataset:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        q = str(r[\"question\"]).strip()\n",
    "        ctx = str(r[\"context\"]).strip()\n",
    "\n",
    "        answers_raw = r[\"answers\"]\n",
    "        try:\n",
    "            ans_list = ast.literal_eval(answers_raw) if isinstance(answers_raw, str) else list(answers_raw)\n",
    "        except Exception:\n",
    "            ans_list = [str(answers_raw)]\n",
    "\n",
    "        for a in ans_list:\n",
    "            rows.append({\n",
    "                \"source\": f\"Pytanie: {q}\\nKontekst: {ctx}\\nOdpowiedź:\",\n",
    "                \"target\": normalize_answer(a)\n",
    "            })\n",
    "\n",
    "    return Dataset.from_list(rows)\n",
    "\n",
    "ds = load_contexts_csv(\"llm_generated_context.csv\")\n",
    "ds = ds.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds, val_ds = ds[\"train\"], ds[\"test\"]\n",
    "\n",
    "len(train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f00639ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Aleksander\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aleksander\\.cache\\huggingface\\hub\\models--allegro--plt5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Map:   0%|          | 0/2554 [00:00<?, ? examples/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Map: 100%|██████████| 2554/2554 [00:00<00:00, 11338.74 examples/s]\n",
      "Map: 100%|██████████| 284/284 [00:00<00:00, 18042.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)\n",
    "\n",
    "def preprocess(batch):\n",
    "    x = tokenizer(batch[\"source\"], max_length=MAX_SOURCE_LEN, truncation=True)\n",
    "    y = tokenizer(text_target=batch[\"target\"], max_length=MAX_TARGET_LEN, truncation=True)\n",
    "    x[\"labels\"] = y[\"input_ids\"]\n",
    "    return x\n",
    "\n",
    "train_tok = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "val_tok = val_ds.map(preprocess, batched=True, remove_columns=val_ds.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d984a3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aleksander\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\peft\\mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Aleksander\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aleksander\\AppData\\Local\\Temp\\ipykernel_21568\\2604573413.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "c:\\Users\\Aleksander\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [480/480 34:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>9.990800</td>\n",
       "      <td>5.829335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.972900</td>\n",
       "      <td>4.431680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aleksander\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zapisano model do: plt5-lora-qa\n"
     ]
    }
   ],
   "source": [
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    target_modules=[\"q\", \"v\"], \n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_cfg)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),   \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(OUT_DIR)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "\n",
    "print(\"Zapisano model do:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb198ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "NUM_RE = re.compile(r\"[-+]?\\d+(?:[.,]\\d+)?\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def answer_question(question: str, context: str) -> str:\n",
    "    prompt = f\"Pytanie: {question}\\nKontekst: {context}\\nOdpowiedź:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SOURCE_LEN).to(device)\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=24,\n",
    "        do_sample=False,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    ans = tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
    "    ans = ans.strip(\"“”\\\"'„”\").rstrip(\" .,:;\")\n",
    "    ans = re.sub(r\"\\s+\", \" \", ans)\n",
    "\n",
    "    m = NUM_RE.search(ans)\n",
    "    if m:\n",
    "        return m.group(0).replace(\",\", \".\")\n",
    "    return ans if ans else \"Nie wiem\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac9eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_df = pd.read_csv(\"llm_generated_context.csv\")\n",
    "q2ctx = dict(zip(ctx_df[\"question\"].astype(str), ctx_df[\"context\"].astype(str)))\n",
    "\n",
    "def get_context_for_question(q: str) -> str:\n",
    "    return q2ctx.get(q, \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "dump_url = \"https://dumps.wikimedia.org/plwiki/latest/plwiki-latest-pages-articles-multistream.xml.bz2\"\n",
    "dump_path = \"plwiki.xml.bz2\"\n",
    "\n",
    "urllib.request.urlretrieve(dump_url, dump_path)\n",
    "print(\"OK:\", dump_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
